{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a250a1b",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62854253",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "This Notebook uses several Python packages. To avoid compatibility issues we recommend running this Notebook in a __virual environment__.\n",
    "- Therefore, install [Anaconda](https://www.anaconda.com/) and follow the installation instructions.\n",
    "- Go to environments and click 'create'. Give your environment a name, for example '__myenv__'. The virual environment will be launched automatically.\n",
    "- Go to the package search bar and search for '__ipywidgets__'. Download the package to be able to use the interactive widgets in this Notebook.\n",
    "- Next, go back to the home page and install __Jupyter Notebook__. Once completed, press launch and go to the directory where you saved this Notebook. \n",
    "- Verify that you see the name of the virtual enivronment on the right top of the Notebook, for example: __Python (myenv)__. If that's not the case, go to Kernel and choose the environment.\n",
    "\n",
    "The workflow uses several non-Python tools ([CD-HIT](https://academic.oup.com/bioinformatics/article/22/13/1658/194225?login=true), [MAFFT](https://academic.oup.com/nar/article/30/14/3059/2904316?login=true) and [FastTree](https://academic.oup.com/mbe/article/26/7/1641/1128976?login=true)). To use this Notebook to its full strength, make sure to download these packages and compile them according to your system. Installation instructions can be found on the [GitHub](https://github.com/PyEED/CANDy) page. Make sure you have them installed in the same directory as this Notebook before running.\n",
    "\n",
    "### Running CANDy\n",
    "\n",
    "If you wish to analyze bigger CAZy families, avoid your computer entering sleep mode since this will interrupt the script. Do this in the settings of your system or by running __caffeinate__ in your Terminal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1bdef",
   "metadata": {},
   "source": [
    "# Input options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2b3db",
   "metadata": {},
   "source": [
    "### CAZy family\n",
    "\n",
    "Pick the CAZy enzyme class and family (and subfamily) you want to analyze from the dropdown menu.\n",
    "\n",
    "### Taxonomy selection\n",
    "\n",
    "If desired, you can perform the actual analysis for a taxonomic subset of the chosen CAZy family (Archaea, Bacteria, Eukaryota, Viruses, Unclassified). \n",
    "\n",
    "### Clustering\n",
    "\n",
    "For a large number of sequences, it is useful to perform a clustering step (CD-HIT). This can significantly reduce the number of input sequences and thus speed up the process. Since CANDy focusses on the occurence of entire protein domains, the clustering cut-off can be set quite low (default: 0.85). \n",
    "\n",
    "### Visualization\n",
    "\n",
    "The found domain architectures can be viualized on a phylogenetic tree, which can be useful for further analysis. When chosen 'Yes' in the dropdown menu, a MSA (MAFFT) and PTI (FastTree) step is performed. This generates a tree file in .nwk format and an [iTOL](https://itol.embl.de/) annotation file containing the domain reprentation of each protein.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb38f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, Label, IntSlider\n",
    "\n",
    "\n",
    "ECdropdown = widgets.Dropdown(\n",
    "    options = [(\"\", \"\"), (\"Glycoside hydrolases\", \"GH\"), (\"Glycosyl transferases\", \"GT\"), (\"Polysaccharide lyases\", \"PL\"), (\"Carbohydrate esterases\", \"CE\"), (\"Auxiliary activities\", \"AA\")])\n",
    "\n",
    "famsize = {\"GH\":180, \"GT\": 116, \"PL\": 42, \"CE\": 20, \"AA\": 17}\n",
    "\n",
    "subfamilies = {'GH5': ['1', '2', '4', '5', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57'],\n",
    "'GH13': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46'],\n",
    "'GH16': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27'],\n",
    "'GH19': ['1', '2'],\n",
    "'GH30': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'],\n",
    "'GH31': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'],\n",
    "'GH43': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '39'],\n",
    "'GH45': ['1', '2', '3', '4'],\n",
    "'GH51': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14'],\n",
    "'GH55': ['1', '2', '3'],\n",
    "'GH68': ['1', '2'],\n",
    "'GH130': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16']}\n",
    "\n",
    "famdropdown = widgets.Dropdown()\n",
    "\n",
    "subfamdropdown = widgets.Dropdown()\n",
    "\n",
    "taxdropdown =  widgets.Dropdown(\n",
    "    options = [(\"All\", \"All\"), (\"Archaea\", \"A\"), (\"Bacteria\", \"B\"), (\"Eukaryota\", \"E\"), (\"Viruses\", \"V\"), (\"Unclassified\", \"U\")])\n",
    "\n",
    "clustervalue = IntSlider(min=0, max=100, step=1, value=85)\n",
    "\n",
    "treedropdown = widgets.Dropdown(options = [(\"No\", False), (\"Yes\", True)])\n",
    "\n",
    "\n",
    "# Define a function to update the options of dropdown2 based on the selected value of dropdown1\n",
    "def update_dropdown2_options(change):\n",
    "    famdropdown.options = [i for i in range(1, famsize[change.new] + 1)]\n",
    "    \n",
    "# Use the observe method of dropdown1 to observe changes to its value and update the options of dropdown2 accordingly\n",
    "ECdropdown.observe(update_dropdown2_options, names='value')\n",
    "\n",
    "family = str(ECdropdown.value) + str(famdropdown.label)\n",
    "\n",
    "def update_dropdown3_options(change):\n",
    "    family = str(ECdropdown.value) + str(change.new)\n",
    "    subfamdropdown.options = ['N.a.']\n",
    "    if family in subfamilies:\n",
    "        subfamdropdown.options += tuple(subfamilies[family])\n",
    "    \n",
    "    \n",
    "famdropdown.observe(update_dropdown3_options, names='value')\n",
    "\n",
    "#print('Enter your e-mail adress: ')\n",
    "email = input('Enter your e-mail adress: ')\n",
    "basejobname = input('Jobname: ')\n",
    "\n",
    "#display the dropdown widgets\n",
    "VBox([Label(\"Choose the enzyme class: \"), ECdropdown, \n",
    "      Label(\"Choose the family number: \"), famdropdown,\n",
    "      Label(\"Choose the subfamily (if applicable): \"), subfamdropdown,\n",
    "      Label(\"Choose a taxonomy subset: \"), taxdropdown,\n",
    "      Label(\"Choose a clustering cut-off (%): \"), clustervalue,\n",
    "      Label(\"Do you wish to construct a phylogenetic tree? \"), treedropdown])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4221231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "family = str(ECdropdown.value) + str(famdropdown.label) + '_' + str(subfamdropdown.label)\n",
    "family = family.strip('_N.a.')\n",
    "taxsubset = taxdropdown.label\n",
    "taxsubsetvalue = taxdropdown.value\n",
    "clustercutoff = clustervalue.value\n",
    "treeconstruction = treedropdown.label\n",
    "execute = treedropdown.value\n",
    "print(f\"Input summary \\n \\n - Chosen CAZy family: {family} \\n - Taxonomy subset: {taxsubset} \\n - Clustering cut-off: {clustercutoff} \\n - Phylogenetic tree construction: {treeconstruction} \")\n",
    "\n",
    "\n",
    "basejobname = \"\".join(basejobname.split())\n",
    "#replaces any non-alphanumeric character\n",
    "basejobname = re.sub(r'\\W+', '', basejobname)\n",
    "jobname = basejobname + f'_{taxsubset}'\n",
    "\n",
    "#check if directory with jobname exists\n",
    "def check(folder):\n",
    "    if os.path.exists(folder):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "if not check(jobname):\n",
    "    n = 0\n",
    "    while not check(f\"{basejobname}_{n}\"): \n",
    "        n += 1\n",
    "    jobname = f\"{basejobname}_{n}\"\n",
    "    \n",
    "# make directory to save results\n",
    "os.makedirs(jobname, exist_ok=True)\n",
    "\n",
    "# save queries\n",
    "queries_path = os.path.join(jobname, f\"{jobname}.csv\")\n",
    "\n",
    "with open(queries_path, \"w\") as text_file:\n",
    "    text_file.write(f\"Jobname,Family,Taxonomy subset,Clustering cut-off, Phylogenetic analysis\\n{jobname},{family}, {taxsubset}, {clustercutoff}, {treeconstruction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845c518",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install Bio\n",
    "!pip3 install pandas\n",
    "!pip3 install requests\n",
    "!pip3 install ete3\n",
    "!pip3 install biopython\n",
    "!pip3 install db-sqlite3\n",
    "!pip3 install SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to use the correct directory of the site-packages. For example: \"/Users/winde/Library/Python/3.8/lib/python/site-packages\"\n",
    "#only run this code if you would have trouble importing certain modules e.g., ete3\n",
    "#change the directory path according to your system\n",
    "\n",
    "#import sys\n",
    "#sys.path.append(\"/Users/winde/Library/Python/3.8/lib/python/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfa400",
   "metadata": {},
   "source": [
    "# Generic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizefastafile (inputfile):\n",
    "    \"\"\"Count the number of protein sequences in a given fasta file\"\"\"\n",
    "    with open (inputfile) as file:\n",
    "        size = 0\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                size += 1\n",
    "                \n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94eaf9a",
   "metadata": {},
   "source": [
    "# Extract CAZy family sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import Bio\n",
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "from urllib.request import HTTPError\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "Entrez.email = email\n",
    "\n",
    "#pandas configuration\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1718e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#script\n",
    "\n",
    "def CAZy_extract(family, data, taxsubset):\n",
    "    \"\"\"This function extracts the GenBank IDs from a CAZy family and builds a FASTA file with corresponding protein\n",
    "    sequences\"\"\"\n",
    "    \n",
    "    print(f'Writing the {family} CAZy data file')\n",
    "    \n",
    "    #write the CAZy ouput to a file\n",
    "    with open(os.path.join(jobname, f\"CAZy_data_{family}.txt\"),'w') as file:\n",
    "        file.write(data)\n",
    "    \n",
    "    #this is the dataframe\n",
    "    df = pd.read_table(os.path.join(jobname, f\"CAZy_data_{family}.txt\"),engine = 'python', header=None)\n",
    "    \n",
    "    #Let's shorten the Taxonomy description\n",
    "    tax_dict = {'BACTERIA': 'B', 'EUKARYOTA': 'E', 'ARCHAEA': 'A', 'VIRUSES': 'V', 'UNCLASSIFIED': 'U'}\n",
    "    \n",
    "    #Create a dictionary with the identifiers as keys and the kingdom as value, e.g., AW1234:E\n",
    "    taxonomydict = {}\n",
    "    \n",
    "    \n",
    "    #go over each row in the df\n",
    "    for index, line in df.iterrows():\n",
    "        \n",
    "        #the kingdom of this identifier is at position 1\n",
    "        tax = line[1].upper()\n",
    "        \n",
    "        #the identifier is at position 3\n",
    "        ID = line[3]\n",
    "        \n",
    "        #build the taxonomydict\n",
    "        taxonomydict[ID] = tax_dict[tax]\n",
    "        \n",
    "     \n",
    "    with open(os.path.join(jobname, f\"Taxonomydict_CAZy_{family}.txt\"), 'w') as file:\n",
    "        file.write(str(taxonomydict))\n",
    "        print('Taxonomydict written')\n",
    "        \n",
    "    #check whether there exist sequences belongling to the chosen taxonomy subset\n",
    "    if taxsubset != 'All' and tax_dict[taxsubset.upper()] not in taxonomydict.values():\n",
    "        print(f'WARNING: Sorry, no sequences belonging to {taxsubset} have been found. Please change your selection at the input options and restart CANDy')\n",
    "        #stop the function\n",
    "        return \n",
    "        \n",
    "    #write the ID's to a separate file\n",
    "    df = df[3]\n",
    "    output = df.to_string(index=False)\n",
    "    \n",
    "    print(f'Writing the {family} GenBank ID file')\n",
    "    with open(os.path.join(jobname, f\"CAZy_{family}_GenBankIDs.txt\"), 'w') as file:\n",
    "        file.write(output)\n",
    "        \n",
    "    with open(os.path.join(jobname, f\"CAZy_{family}_GenBankIDs.txt\"), 'r') as file:\n",
    "        total_length = len(file.readlines()) #can't be integrated in the next lines: tqdm(file, total) because files gets closed\n",
    "\n",
    "    print(f'Extracting protein sequences from NCBI')\n",
    "    \n",
    "    #fetch the protein sequences using Entrez.efetch\n",
    "    fastasequences = ''\n",
    "    with open(os.path.join(jobname, f\"CAZy_{family}_GenBankIDs.txt\"), 'r') as file:\n",
    "        for line in tqdm(file, total = total_length):\n",
    "            \n",
    "            #take into account the selected taxonomic subset\n",
    "            if taxonomydict[line.strip()] == taxsubsetvalue if taxsubsetvalue != 'All' else (True):\n",
    "                attempts = 0\n",
    "\n",
    "                #during peak hours, a HTTP out error may occur. Try 10 times untill going to the next ID\n",
    "                while attempts < 10:\n",
    "                    try:\n",
    "\n",
    "                        #fetch the sequences\n",
    "                        handle = Entrez.efetch(\"Protein\", id = line, retmode = \"text\", rettype = \"fasta\")\n",
    "                        try:\n",
    "                            seq_record = SeqIO.read(handle, \"fasta\")\n",
    "                        except:\n",
    "                            print(\"Something went wrong, let's skip this one.\")\n",
    "                            break  # Something went wrong retrieving data\n",
    "\n",
    "                        thisoutput = \">\"+seq_record.description+\"\\n\"+str(seq_record.seq)+\"\\n\"\n",
    "                        fastasequences = fastasequences + thisoutput\n",
    "                        handle.close()\n",
    "                        break\n",
    "\n",
    "                    except HTTPError:\n",
    "                        #HTTP Error. Let's try waiting for a little bit\n",
    "                        attempts += 1\n",
    "                        sleep(3)\n",
    "\n",
    "    return fastasequences, taxonomydict\n",
    "\n",
    "#create an outputfile\n",
    "\n",
    "\n",
    "print(f'Working on family {family}')\n",
    "#url\n",
    "url = 'http://www.cazy.org/IMG/cazy_data/'+family+'.txt'\n",
    "print(f'Retrieving data from page {url}')\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "\n",
    "#extract GenBank ID's from this page\n",
    "\n",
    "sequence_extraction = CAZy_extract(family,data, taxsubset)\n",
    "result = sequence_extraction[0]\n",
    "taxonomydict = sequence_extraction[1]\n",
    "\n",
    "print(f'Writing {family}_{taxsubset} FASTA sequences file')\n",
    "with open(os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_FASTA_sequences.fasta\"),'w') as file:\n",
    "    file.write(result)\n",
    "\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f27039",
   "metadata": {},
   "source": [
    "# Exclude sequences not available in UniParc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc41f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import hashlib\n",
    "import urllib\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#inputfile\n",
    "inputfile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_FASTA_sequences.fasta\")\n",
    "\n",
    "#outputfile\n",
    "outputfile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_Verified_FASTA_sequences.fasta\")\n",
    "\n",
    "#API url\n",
    "URL = \"https://www.ebi.ac.uk/interpro/match-lookup/matches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we generate a file only containing UNIQUE sequences that can be used for InterPro domain detection\n",
    "def parse_fasta(inputfile):\n",
    "    \"\"\"This function parses trough a given FASTA file and creates a dict of ID: sequence\"\"\"\n",
    "    proteins = {}\n",
    "    \n",
    "    with open(inputfile, 'r') as file:\n",
    "        count = 0\n",
    "        for seq_record in SeqIO.parse(file, \"fasta\"):\n",
    "            proteins[seq_record.id] = str(seq_record.seq)\n",
    "            count += 1\n",
    "            \n",
    "    return proteins\n",
    "\n",
    "def split_dict(dictionary, n):\n",
    "    dict_list = np.array_split(list(dictionary.items()), n)\n",
    "    return [dict(item) for item in dict_list]\n",
    "\n",
    "def check_uniparc(proteins, print_output = True):\n",
    "    \"\"\"This function checks which sequences have a UniParc ID and thus can be used for InterPro domain detection\"\"\"\n",
    "    \n",
    "    md5_to_id = {}\n",
    "    print(\"Checking UniParc ID's.\")\n",
    "    #converts the protein sequences to a hexadecimal representation in uppercase\n",
    "    for identifier, sequence in proteins.items():\n",
    "        md5 = hashlib.md5(sequence.encode()).hexdigest().upper()\n",
    "        #if the sequences are 100% identical the old identifier gets removed... \n",
    "        if md5 not in md5_to_id:\n",
    "            md5_to_id[md5] = identifier\n",
    "        else:\n",
    "            md5_to_id[md5] += ' ' + identifier\n",
    "    \n",
    "    #print(md5_to_id)\n",
    "    md5_to_id_len = len(md5_to_id)   \n",
    "    \n",
    "    if md5_to_id_len > 5000:\n",
    "        div = md5_to_id_len//5000 #we want to send the sequences in packages of 5000\n",
    "        md5_to_id_list = split_dict(md5_to_id, div) #this will return a list of dictionaries\n",
    "        \n",
    "    else:\n",
    "        md5_to_id_list = [md5_to_id]\n",
    "    \n",
    "    domain_ids = []\n",
    "    unknown_ids = []\n",
    "    \n",
    "    for subdictionary in md5_to_id_list:\n",
    "        \n",
    "        attempts = 0\n",
    "        \n",
    "        #params is a dictionary that contains all the sequences in hexadecimal representation\n",
    "        params = {\"md5\": list(subdictionary.keys())}\n",
    "        #print(params)\n",
    "        data = urlencode(params, doseq=True).encode(\"ascii\")\n",
    "        #print(data)\n",
    "    \n",
    "        #while attempts < 10:\n",
    "        while True:\n",
    "            try:\n",
    "                with urlopen(URL, data) as res:\n",
    "                    data = res.read().decode(\"utf-8\")\n",
    "                    break\n",
    "                \n",
    "            except urllib.error.HTTPError as e:\n",
    "                attempts += 1\n",
    "                print('Retrying in 5 seconds.')\n",
    "                time.sleep(5)\n",
    "            \n",
    "\n",
    "        root = ET.fromstring(data)\n",
    "        for m in root.findall(\".//match\"):\n",
    "            md5 = m.find(\"proteinMD5\").text\n",
    "        \n",
    "            #The pop() method removes the item at the given index from the list and returns the removed item\n",
    "            protein_id = subdictionary.pop(md5) \n",
    "        \n",
    "            for identifier in protein_id.split(' '):\n",
    "                domain_ids.append(identifier)\n",
    "\n",
    "\n",
    "        for protein_id in subdictionary.values():\n",
    "            \n",
    "            #for these sequences no match could be found\n",
    "            for identifier in protein_id.split(' '):\n",
    "                unknown_ids.append(identifier)\n",
    "                \n",
    "    if print_output: \n",
    "        print(f'{len(domain_ids)} out of {len(domain_ids) + len(unknown_ids)} sequences will be included.')\n",
    "        print(f'{len(unknown_ids)} sequences can not be found in UniParc and will be excluded. \\n' )\n",
    "    return domain_ids, unknown_ids\n",
    "\n",
    "def verified_sequences(domain_ids, inputfile):\n",
    "    output = ''\n",
    "\n",
    "    with open(inputfile, 'r') as file:\n",
    "        for seq_record in SeqIO.parse(file, \"fasta\"):\n",
    "\n",
    "            if str(seq_record.id) in domain_ids:\n",
    "                output += '>' + str(seq_record.description) + '\\n' + str(seq_record.seq) + '\\n'\n",
    "         \n",
    "    with open (outputfile, 'w') as f:\n",
    "        f.write(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "uniparccheck = check_uniparc(parse_fasta(inputfile))\n",
    "verseq = verified_sequences(uniparccheck[0], inputfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ad44d",
   "metadata": {},
   "source": [
    "# Format FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTION A fasta file will be formatted to remove duplicate sequences, and to remove duplicate identifiers.\n",
    "\n",
    "# EXAMPLES\n",
    "# >gi|2660639|emb|CAA04523.1| alpha-glucan phosphorylase [Thermotoga maritima]\n",
    "# will be changed to\n",
    "# >2660639\n",
    "# \n",
    "# >gi|754097242|ref|WP_041736169.1| alpha-glucan phosphorylase [Coprothermobacter proteolyticus] >gi|639380266|gb|ACI17870.2| alpha-glucan phosphorylase [Coprothermobacter proteolyticus DSM 5265]\n",
    "# will be changed to\n",
    "# >754097242\n",
    "# \n",
    "# And if multiple identifiers contain the exact same sequence, only one is retained.\n",
    "\n",
    "import ast\n",
    "\n",
    "# Route to the fasta files\n",
    "fastafile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_Verified_FASTA_sequences.fasta\")\n",
    "outputfile_name = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_FASTA_sequences_formatted.fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f506853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fasta(fastafile, taxsubsetvalue):\n",
    "    \"\"\"This function file formats an input fasta file to detelete the description \n",
    "    and special characters in the header\"\"\"\n",
    "\n",
    "    sequence_list = []\n",
    "    id_list = []\n",
    "\n",
    "    output = ''\n",
    "\n",
    "    with open(fastafile,'r') as file,  open(os.path.join(jobname, f\"Taxonomydict_CAZy_{family}.txt\"), 'r') as r:\n",
    "        taxonomydict = ast.literal_eval(r.read())\n",
    "\n",
    "        for seq_record in SeqIO.parse(file, \"fasta\"):\n",
    "            if seq_record.seq not in sequence_list:\n",
    "                sequence_list.append(seq_record.seq)\n",
    "                if seq_record.id not in id_list:\n",
    "\n",
    "                    sequenceidentifier = seq_record.id\n",
    "\n",
    "                    id_list.append(sequenceidentifier)\n",
    "\n",
    "                    desc = seq_record.description\n",
    "                    \n",
    "                    #if there's no organism in the description, stop and continue with the next sequence\n",
    "                    if desc.find('[') == -1:\n",
    "                        #remove the sequence from the list again\n",
    "                        sequence_list.remove(seq_record.seq)\n",
    "                        continue\n",
    "                        \n",
    "                    tax = desc[desc.find('[') + 1:desc.rfind(']')]\n",
    "\n",
    "                    #in case of: [[Candida] auris]\n",
    "                    if tax.find('[') != -1 and tax.find(']') != -1:\n",
    "                        tax = tax[tax.find('[') + 1:tax.find(']')] + tax[tax.find(']') + 1:]\n",
    "                        \n",
    "                        \n",
    "                    #if there are still other special characters, just remove them:\n",
    "                    tax = tax.replace(' ', '_')\n",
    "                    tax = re.sub(r'\\W+', '', tax)\n",
    "\n",
    "                    #sometimes the tax is weird (NCBI?) description. Just shorten the tax \n",
    "\n",
    "                    if len(tax) > 100:\n",
    "                        tax = tax[:100]\n",
    "\n",
    "                    #if the last character is a '_' it will be removed from the string\n",
    "                    tax = tax.rstrip('_')\n",
    "\n",
    "                    #ID's like AW_12345 will be converted to AW12345\n",
    "                    if sequenceidentifier.find('_') != -1:\n",
    "                        sequenceidentifier = sequenceidentifier.replace('_', '')\n",
    "\n",
    "                    output += '>' + sequenceidentifier + '_' + tax + '_' + taxonomydict[seq_record.id] + '\\n' + seq_record.seq + '\\n'\n",
    "            \n",
    "    \n",
    "    #write the formatted sequences to the outputfile\n",
    "    with open(outputfile_name, 'w') as f:\n",
    "        f.write(str(output))\n",
    "    \n",
    "    return output\n",
    "\n",
    "formatted_file = format_fasta(fastafile, taxsubsetvalue)\n",
    "\n",
    "with open(outputfile_name, 'w') as f:\n",
    "    f.write(str(formatted_file))\n",
    "        \n",
    "numberofseq = sizefastafile(outputfile_name)\n",
    "print(f'Only sequences belonging to {taxsubset} will be included and used for further analysis.' if taxsubset != 'All' else '')\n",
    "print(f'{numberofseq} unique sequences were obtained after formatting.')\n",
    "    \n",
    "print('Finished!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b5982",
   "metadata": {},
   "source": [
    "# Clustering sequences: CD-HIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering cut-off chosen at input options \n",
    "#name of the input file\n",
    "inputfastafile = os.path.join(outputfile_name)\n",
    "\n",
    "#name of the output file\n",
    "cutoff = \"{0:.0%}\".format(clustercutoff/100)\n",
    "outputfastafile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_formatted_{cutoff}.fasta\")\n",
    "\n",
    "#path of cd-hit exe\n",
    "\n",
    "cdhit = os.getcwd() + '/cd-hit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d9c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run CD-HIT\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "cdhit_process = Popen([cdhit, \"-i\", inputfastafile, \"-o\", outputfastafile, \"-c\", str(clustercutoff/100)], stdout=PIPE, stderr=PIPE)\n",
    "stdout, stderr = cdhit_process.communicate()\n",
    "\n",
    "print(f\"Number of sequences of the inputfile: {sizefastafile(inputfastafile)} \")\n",
    "print(f\"Number of sequences of the outputfile after clustering at {clustercutoff}%: {sizefastafile(outputfastafile)}\")\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcce47",
   "metadata": {},
   "source": [
    "# Include characterized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac399c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script checks if characterized enzymes are retained in the file after clustering and adds the full description of the characterized enzymes\n",
    "\n",
    "#name of the inputfile\n",
    "\n",
    "inputfile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_formatted_{cutoff}.fasta\")\n",
    "outputfile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_inclchar.fasta\")\n",
    "\n",
    "#Following files will be saved locally:\n",
    "# 1. A TXT file with the characterized GenBANK ID's from the CAZy family\n",
    "# 2. A FASTA file with the characterized FASTA sequences corresponding to the ID's\n",
    "# 3. A FASTA file containing the clustered sequences WITH all characterized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary modules\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import requests\n",
    "import Bio\n",
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "from urllib.request import HTTPError\n",
    "from tqdm import tqdm\n",
    "\n",
    "#pandas configuration\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "Entrez.email = email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ef65a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fetch_pages(family):\n",
    "    \"\"\"This function fetches the number of pages with characterized enzymes for this CAZy family\"\"\"\n",
    "    #The servers easily get overloaded, resulting in a HTTPError 429. Therefore, keep on trying with 5s waiting time.\n",
    "    \n",
    "    url =  'http://www.cazy.org/'+family+'_characterized.html#pagination_FUNC'\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_html(url)[1][4]\n",
    "            break\n",
    "        except HTTPError:\n",
    "            print(\"Too many requests. Let's wait for a little bit.\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    if str(df.iloc[0]) != 'nan':\n",
    "        pages = df.iloc[0].count('|') - 1\n",
    "\n",
    "    else:\n",
    "        pages = 1\n",
    "    \n",
    "    return pages\n",
    "    \n",
    "def extract_characterized(url):\n",
    "    \"\"\"Extract GenBank ID's of characterized proteins from CAZy\"\"\"\n",
    "    #The servers easily get overloaded, resulting in a HTTPError 429. Therefore, keep on trying with 5s waiting time.\n",
    "    while True:\n",
    "        try:\n",
    "            df = pd.read_html(url)[1][4]\n",
    "            time.sleep(5)\n",
    "            dfact = pd.read_html(url)[1]\n",
    "            break\n",
    "        except HTTPError:\n",
    "            print(\"Too many requests. Let's wait for a little bit.\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    #create a dictionary containing the EC numbers of the characterized enzymes        \n",
    "    activitydictionary = {}\n",
    "    for i in range(len(dfact)):\n",
    "        identifier = str(dfact[4].loc[i])\n",
    "        if identifier.find('.') != -1:\n",
    "            activitydictionary[identifier] = dfact[1].loc[i] #EC number\n",
    "        \n",
    "    #drop duplicate identifiers\n",
    "    df = df.drop_duplicates()\n",
    "    output = df.to_list()\n",
    "    \n",
    "    #create a list containing the identifiers of the characterized enzymes\n",
    "    id_list = ''\n",
    "    \n",
    "    #fetch the taxonomy of every characterized sequence. This is necessary to be able to include only seq by specified tax\n",
    "    txdct = {\"Archaea\": \"A\", \"Bacteria\": \"B\", \"Eukaryota\": \"E\", \"Viruses\": \"V\", \"Unclassified\": \"U\"}\n",
    "    tx = ''\n",
    "    #create a dictionary of type 'identifier':tax e.g., 'AW1234.1':'E'\n",
    "    chartaxdct = {}\n",
    "    \n",
    "    for element in output:\n",
    "        element = str(element)\n",
    "        pointposition = element.find('.')\n",
    "        \n",
    "        #fetch tax\n",
    "        if element in txdct:\n",
    "            tx = txdct[element]\n",
    "            \n",
    "        elif pointposition != -1:\n",
    "            element = element[:pointposition + 2]\n",
    "            if element not in id_list:\n",
    "                id_list += element + '\\n'\n",
    "                chartaxdct[element] = tx\n",
    "\n",
    "    return id_list, chartaxdct, activitydictionary\n",
    "\n",
    "def CAZy_extract(characterized_ID, characterizedtaxdict):\n",
    "    \"\"\"Convert extracted GenBank ID's to FASTA sequences\"\"\"\n",
    "    \n",
    "    data = characterized_ID\n",
    "    chartaxdct = characterizedtaxdict\n",
    "    total_length = data.count('\\n')\n",
    "\n",
    "\n",
    "    print(f'Extracting FASTA sequences from NCBI')\n",
    "    fastasequences = ''\n",
    "    with open(os.path.join(jobname, f\"Characterized_{family}_GenBankID_file.txt\"), 'r') as file:\n",
    "        for line in tqdm(file, total = total_length):\n",
    "            \n",
    "            #take into account the selected taxonomic subset\n",
    "            if chartaxdct[line.strip()] == taxsubsetvalue if taxsubsetvalue != 'All' else (True):\n",
    "                try:\n",
    "                    handle = Entrez.efetch(\"Protein\", id = line, retmode = \"text\", rettype = \"fasta\")\n",
    "                    try:\n",
    "                        seq_record = SeqIO.read(handle, \"fasta\")\n",
    "                    except:\n",
    "                        print(\"Something went wrong, let's skip this one.\")\n",
    "                        break  # Something went wrong retrieving data\n",
    "\n",
    "                    thisoutput = \">\"+seq_record.id[0:str(seq_record).find('.')+1] + \" \"+ seq_record.description[str(seq_record.description).find('[')+1:str(seq_record.description).find(']')]+'_'+chartaxdct[line.strip()]+\"\\n\"+str(seq_record.seq)+ \"\\n\"\n",
    "                    thisoutput = thisoutput.replace(' ','_')\n",
    "                    fastasequences = fastasequences + thisoutput\n",
    "                    handle.close()\n",
    "                except HTTPError:\n",
    "                    print(\"HTTP Error. Let's try waiting for a little bit.\")\n",
    "                    #time.sleep(1)\n",
    "\n",
    "    return fastasequences\n",
    "\n",
    "characterized_ID = ''\n",
    "characterizedtaxdict = {}\n",
    "characterizedECnumbers = {}\n",
    "\n",
    "#retrieve the number of pages containing characterized enzymes\n",
    "pages = fetch_pages(family)\n",
    "for page in range(pages):\n",
    "    url = 'http://www.cazy.org/'+family+'_characterized.html?debut_FUNC='+str(page)+'00#pagination_FUNC'\n",
    "    print(f'Retrieving data from page {url}')\n",
    "    fetchsequences = extract_characterized(url)\n",
    "    characterized_ID += fetchsequences[0]\n",
    "    characterizedtaxdict.update(fetchsequences[1])\n",
    "    characterizedECnumbers.update(fetchsequences[2])\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f'Writing {family} Characterized GenBank ID file')\n",
    "with open(os.path.join(jobname, f\"Characterized_{family}_GenBankID_file.txt\"), 'w') as file:\n",
    "    file.write(characterized_ID)\n",
    "\n",
    "characterized_sequences = CAZy_extract(characterized_ID, characterizedtaxdict)\n",
    "print(f'Writing {family}_{taxsubset} Characterized FASTA sequences file')\n",
    "with open(os.path.join(jobname, f\"Characterized_{family}_{taxsubset}_FASTA_sequences.fasta\"), 'w') as file:\n",
    "    file.write(characterized_sequences)\n",
    "\n",
    "    print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdca1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that the GenBank IDs of the characterized sequences are included in the uniparccheck list \n",
    "\n",
    "verified_IDs = []\n",
    "no_uniparc = []\n",
    "\n",
    "with open(os.path.join(jobname, f\"Characterized_{family}_{taxsubset}_FASTA_sequences.fasta\"), 'r') as file:\n",
    "    for seq_record in SeqIO.parse(file,'fasta'):\n",
    "        identifier = seq_record.id[:seq_record.id.find('.')+2]\n",
    "        if identifier in uniparccheck[0]:\n",
    "            print(f'{identifier}: UniParc ID found')\n",
    "            verified_IDs.append(identifier)\n",
    "            \n",
    "        else:\n",
    "            no_uniparc.append(identifier)\n",
    "            print(f'Warning: No UniParc entry not found for {identifier}. A BLAST step will be performed.')\n",
    "            \n",
    "print('Finished!')            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c990fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Blast import NCBIWWW\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "infile = os.path.join(jobname, f\"Characterized_{family}_{taxsubset}_FASTA_sequences.fasta\")\n",
    "\n",
    "def blast_char_id(infile, no_uniparc, verified_IDs):\n",
    "    #default identity threshold is 90%. Anything lower may result in different domain compositions.\n",
    "    identity_threshold = 90\n",
    "    E_value_tresh = 1e-20 \n",
    "\n",
    "    #output a dictionary with characterized GenBank ID's that we found an appropriate Uniprot ID for using BLASTP\n",
    "    outputblastdict = {}\n",
    "    outputchar = []\n",
    "    charblastdict = {}\n",
    "\n",
    "    with open (infile, 'r') as file:\n",
    "        #go over each protein sequence in the characterized fasta file\n",
    "        for seq_record in SeqIO.parse(file, \"fasta\"):\n",
    "            ID = seq_record.id\n",
    "            ID = ID[:ID.find('_')]\n",
    "\n",
    "            #if the ID is also present in the list of characterized ID's without corresponding Uniprot ID, blast the sequence\n",
    "            if ID in no_uniparc:\n",
    "            #if ID == 'AAL81357.1':\n",
    "                charidlist = []\n",
    "                identitydict = {}\n",
    "                print(seq_record.id)\n",
    "                print('Doing a BLASTP search.')\n",
    "                result_handle = NCBIWWW.qblast(\"blastp\", \"nr\", seq_record.seq, expect=5.0, word_size=6)\n",
    "                print('Going over BLAST results.')\n",
    "\n",
    "                #go over the blast results\n",
    "                blast_records = NCBIXML.parse(result_handle)\n",
    "                #print(blast_records)\n",
    "                for blast_record in blast_records:\n",
    "                    #print(blast_record)\n",
    "                    for alignment in blast_record.alignments:\n",
    "                        #print(alignment)\n",
    "                        for hsp in alignment.hsps:\n",
    "                            proteinsequence = hsp.sbjct\n",
    "                            #print(proteinsequence)\n",
    "                            identity = hsp.identities / hsp.align_length * 100\n",
    "\n",
    "                            if identity >= identity_threshold:\n",
    "                                #add this id to list\n",
    "                                charidlist.append(alignment.accession)\n",
    "                                #add this id with corresponding identity to dict\n",
    "                                identitydict[alignment.accession] = float(\"{:.2f}\".format(identity))\n",
    "                                outputblastdict[alignment.accession] = proteinsequence\n",
    "\n",
    "                #use the InterPro API to check for UniParc matches. check_uniparc takes a dict as input\n",
    "                #print(outputblastdict)\n",
    "                inuniparc = []\n",
    "                if len(outputblastdict) != 0:\n",
    "                    inuniparc = check_uniparc(outputblastdict, print_output = False)[0]\n",
    "        \n",
    "                #create a new dictionary with matches:identity for matches that have a UniParc ID\n",
    "                newiddict = {}\n",
    "        \n",
    "                #if there is a UniParc identifier for the found BLAST matches:\n",
    "                if len(inuniparc) != 0:\n",
    "                    for ele in inuniparc:\n",
    "                            if ele in identitydict:\n",
    "                                newiddict[ele] = identitydict[ele]\n",
    "\n",
    "                    #use the match with highest identity to the query\n",
    "                    maxid = max(newiddict, key = newiddict.get)\n",
    "\n",
    "                    #add the ID to the verified_IDs list\n",
    "                    verified_IDs.append(ID)\n",
    "                    charblastdict[ID] = maxid\n",
    "                    print(f'For characterized sequence {ID}, {maxid} ({newiddict[maxid]}% identical) will be used for domain-detection. \\n')\n",
    "\n",
    "                else:\n",
    "                    print(f'Sorry, no good match could be found for {ID}. ')\n",
    "\n",
    "\n",
    "    return charblastdict, outputblastdict, verified_IDs\n",
    " \n",
    "blastchar = blast_char_id(infile, no_uniparc, verified_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#following code checks if the ID's of characterized enzymes are present in the FASTA sequences file. If so it removes the ID and sequence\n",
    "#afterwards all ID's and sequences of characterized enzymes are added at the end of the file\n",
    "print('Creating a file containing clustered sequences and all characterized sequences with corresponding CAZy description')\n",
    "verified_IDs = blastchar[2]\n",
    "with open(outputfile,'w') as f:\n",
    "    for seq in SeqIO.parse(inputfile,'fasta'):\n",
    "        \n",
    "        if seq.id.find('_') >2: \n",
    "            proteinidentifier = seq.id.split('_')[0]\n",
    "        else:\n",
    "            proteinidentifier = seq.id.split('_')[0]+'_'+seq.id.split('_')[1]\n",
    "        \n",
    "        if proteinidentifier not in verified_IDs:\n",
    "\n",
    "            SeqIO.write(seq,f,\"fasta\")\n",
    "   \n",
    "    charoutput = ''\n",
    "    #Let's add the characterized enzymes. Take into account the chosen taxonomy subset\n",
    "    with open (os.path.join(jobname, f\"Characterized_{family}_{taxsubset}_FASTA_sequences.fasta\"),'r') as r:\n",
    "    \n",
    "        \n",
    "        for seq_record in SeqIO.parse(r, 'fasta'):\n",
    "            #extract the protein ID from the header\n",
    "            if seq_record.id.find('_') >2: \n",
    "                proteinidentifier = seq_record.id.split('_')[0]\n",
    "                \n",
    "            #if there is and '_' in the identifier, we need to combine the two parts after splitting them    \n",
    "            else:\n",
    "                proteinidentifier = seq_record.id.split('_')[0]+'_'+seq_record.id.split('_')[1]\n",
    "                \n",
    "            if proteinidentifier in verified_IDs and seq_record.id[-1] == taxsubsetvalue if taxsubsetvalue != 'All' else proteinidentifier in verified_IDs:\n",
    "                #ID's like AW_12345 will be converted to AW12345\n",
    "                if proteinidentifier.find('_') != -1:\n",
    "                    protid = proteinidentifier.replace('_', '')\n",
    "                \n",
    "                #all other IDs remain the same\n",
    "                else:\n",
    "                    protid = proteinidentifier\n",
    "                \n",
    "                #we need to add the description again\n",
    "                if seq_record.id.find('_') >2: \n",
    "                    desc = \"_\".join(seq_record.id.split(\"_\", 1)[1:])\n",
    "                    \n",
    "                else:\n",
    "                    desc = \"_\".join(seq_record.id.split(\"_\", 2)[2:])\n",
    "                    \n",
    "                #for the characterised ID's we did a succesful BLAST search for:\n",
    "                if proteinidentifier in blastchar[0]:\n",
    "\n",
    "                    charoutput += '>' + protid+ '_' + desc + '\\n' + blastchar[1][blastchar[0][proteinidentifier]]+ '\\n'\n",
    "                    \n",
    "                #for all other characterised ID's\n",
    "                else:\n",
    "\n",
    "                    charoutput += '>' + protid + '_' + desc + '\\n' + seq_record.seq + '\\n'\n",
    "                \n",
    "\n",
    "    print('Writing '+ outputfile + '\\n' )\n",
    "    f.write(str(charoutput))\n",
    "    \n",
    "with open(outputfile,'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "\n",
    "print(f\"Number of sequences of the outputfile after including characterized sequences: {sizefastafile(outputfile)}\") \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1d11c",
   "metadata": {},
   "source": [
    "# Domain detection: InterPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29477d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import urllib\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from time import sleep\n",
    "import json\n",
    "import sys, errno, re, json, ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfa76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we generate a file only containing UNIQUE sequences that can be used for InterPro domain detection\n",
    "def prep_input(inputfastafile, charblastdict, outputblastdict):\n",
    "    \"\"\"This function parses trough a given FASTA file and creates a dict of ID: sequence\"\"\"\n",
    "    proteins = {}\n",
    "    \n",
    "    with open(inputfastafile,'r') as f:\n",
    "        for seq_record in SeqIO.parse(f, 'fasta'):\n",
    "            seq_id = seq_record.id[:seq_record.id.find('_')]\n",
    "            \n",
    "            #if we did a BLAST search for characterized proteins, we'll use the sequence of the result\n",
    "            if seq_id in charblastdict:\n",
    "                proteins[seq_id] = outputblastdict[charblastdict[seq_id]]\n",
    "            else:\n",
    "                proteins[seq_id] = str(seq_record.seq)\n",
    "    \n",
    "    return proteins\n",
    "\n",
    "def domain_detection(inputfastafile, charblastdict, outputblastdict):\n",
    "    \"\"\"This function uses the InterPro API and returns the domains that can be found in several databases\"\"\"\n",
    "    \n",
    "    md5_to_id = {}\n",
    "    \n",
    "    domainoutput = {}\n",
    "    positionoutput = {}\n",
    "    databaseoutput = {}\n",
    "    \n",
    "    #create a dictionary of ID:sequence\n",
    "    \n",
    "    proteins = prep_input(fastaseq, blastchar[0], blastchar[1])\n",
    "    #converts the protein sequences to a hexadecimal representation in uppercase\n",
    "    for identifier, sequence in proteins.items():\n",
    "        md5 = hashlib.md5(sequence.encode()).hexdigest().upper()\n",
    "        md5_to_id[md5] = identifier\n",
    "    \n",
    "        \n",
    "    #print(md5_to_id)\n",
    "    params = {\"md5\": list(md5_to_id.keys())}\n",
    "    #print(params)\n",
    "    data = urlencode(params, doseq=True).encode(\"ascii\")\n",
    "    #print(data)\n",
    "    \n",
    "    with urlopen(URL, data) as res:\n",
    "        data = res.read().decode(\"utf-8\")\n",
    "    \n",
    "    domain_ids = []\n",
    "    unknown_ids = []\n",
    "    \n",
    "\n",
    "    root = ET.fromstring(data)\n",
    "    for m in root.findall(\".//match\"):\n",
    "        md5 = m.find(\"proteinMD5\").text\n",
    "        protein_id = md5_to_id.pop(md5) #ID's of proteins that have a UniParc Identifier\n",
    "        domain_ids.append(protein_id)\n",
    "        \n",
    "        #print(protein_id)\n",
    "        domaindictionary = {}\n",
    "        positionlist = []\n",
    "        databaselist = []\n",
    "        \n",
    "        #find domains on InterPro\n",
    "        for hit in m.findall(\"hit\"):\n",
    "\n",
    "            #print(hit)\n",
    "            columns = hit.text.split(\",\")\n",
    "            \n",
    "            fragments = []\n",
    "            for frag in columns[6].split(\";\"):\n",
    "                start, end, _ = frag.split(\"-\")\n",
    "                fragments.append((int(start), int(end)))\n",
    "            \n",
    "                \n",
    "                domaindatabase = columns[0]\n",
    "                domainname = columns[2]\n",
    "                domainstart = fragments[0][0]\n",
    "                domainend = fragments[0][1]\n",
    "            \n",
    "            domaindictionary[str([domainstart, domainend])] = domainname\n",
    "            positionlist.append([domainstart, domainend])\n",
    "            databaselist.append([domaindatabase, domainname])\n",
    "            \n",
    "\n",
    "        domainoutput[protein_id] = domaindictionary\n",
    "        positionoutput[protein_id] = positionlist\n",
    "        databaseoutput[protein_id] = databaselist\n",
    "\n",
    "    \n",
    "    for protein_id in md5_to_id.values():\n",
    "        unknown_ids.append(protein_id)\n",
    "        \n",
    "        sys.stderr.write(f\"No domains found in {protein_id}:\"f\" try to run InterProScan\\n\")\n",
    "        \n",
    "    print(domainoutput)\n",
    "    print('\\n')\n",
    "    print(positionoutput)\n",
    "    print('\\n')\n",
    "    print(databaseoutput)\n",
    "    \n",
    "    return domainoutput, positionoutput, databaseoutput\n",
    "\n",
    "\n",
    "#inputfile\n",
    "fastaseq = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_inclchar.fasta\")\n",
    "\n",
    "#fetch the domains\n",
    "finddomains = domain_detection(fastaseq, blastchar[0], blastchar[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from operator import itemgetter\n",
    "# standard library modules\n",
    "import sys, errno, re, json, ssl\n",
    "import urllib\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from time import sleep\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_name(databaseoutput):\n",
    "    \"\"\"This function retrieves the appropriate name for each found domain. it converts the InterPro ID to the corresponding name\"\"\"\n",
    "    \n",
    "    ROOTURL = 'https://www.ebi.ac.uk/interpro/api/entry/'\n",
    "    entryDB = {'GENE3D':'cathgene3d', 'CDD':'cdd', 'HAMAP':'hamap', 'PANTHER':'panther', 'PFAM':'pfam', 'PIRSF':'pirsf', 'PRINTS':'prints', 'PROSITE_PROFILES':'profile', 'PROSITE_PATTERNS':'prosite', 'SFLD':'sfld', 'SMART':'smart', 'SUPERFAMILY':'ssf', 'TIGRFAMS':'tigrfams'}\n",
    "    \n",
    "    #output\n",
    "    domainnamelist = {}\n",
    "    domainnamedictionary = {}\n",
    "    \n",
    "    #let's speed up the proces by creating a dictionary: 'domain code': 'domain name'\n",
    "    domaincode_name_dict = {}\n",
    "    \n",
    "    total_length = len(databaseoutput)\n",
    "    for key, value in tqdm(databaseoutput.items(), total = total_length):\n",
    "        templist = []\n",
    "        tempdict = {}\n",
    "        \n",
    "        for domain in value:\n",
    "            \n",
    "        #database = domain[0]\n",
    "        #domain code = domain[1]\n",
    "        \n",
    "            #first, check if the domain code is already present in the domaincode_name_dict, if so fetch the name\n",
    "            if domain[1] in domaincode_name_dict:\n",
    "                #fetch the name\n",
    "                name = domaincode_name_dict[domain[1]]\n",
    "                    \n",
    "                #add the name to a dictionary and list\n",
    "                tempdict[name] = domain[0]  \n",
    "                templist.append([domain[0], name])\n",
    "                \n",
    "            #if this domain code is not yet present in domaincode_name_dict, use the API to fetch and append it     \n",
    "            else:\n",
    "        \n",
    "                if domain[0] in entryDB:\n",
    "\n",
    "                    #disable SSL verification to avoid config issues\n",
    "                    context = ssl._create_unverified_context()\n",
    "\n",
    "                    next = ROOTURL + entryDB[domain[0]] + '/' + domain[1]\n",
    "                    last_page = False\n",
    "                    attempts = 0\n",
    "\n",
    "                    while next:\n",
    "                        try:\n",
    "\n",
    "                            req = request.Request(next, headers={\"Accept\": \"application/json\"})\n",
    "                            res = request.urlopen(req, context=context)\n",
    "                            # If the API times out due a long running query\n",
    "                            if res.status == 408:\n",
    "                            # wait just over a minute\n",
    "                                sleep(61)\n",
    "                            # then continue this loop with the same URL\n",
    "                                continue\n",
    "\n",
    "                            elif res.status == 204:\n",
    "                                print('sorry no data could be retrieved for this entry' + '\\n')\n",
    "                                break\n",
    "\n",
    "                            elif res.status == 404:\n",
    "                                print('sorry no data could be retrieved for this entry' + '\\n')\n",
    "                                break\n",
    "\n",
    "                            payload = json.loads(res.read().decode())\n",
    "                            attempts = 0\n",
    "\n",
    "                            last_page = True\n",
    "                            next = False #needed to avoid loop\n",
    "\n",
    "                        except HTTPError as e:\n",
    "                            if e.code == 408:\n",
    "                                sleep(61)\n",
    "                                continue\n",
    "                            else:\n",
    "                                #If there is a different HTTP error, it wil re-try 3 times before failing\n",
    "                                if attempts < 3:\n",
    "                                    attempts += 1\n",
    "                                    sleep(61)\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    sys.stderr.write(\"LAST URL: \" + next)\n",
    "                                    raise e\n",
    "\n",
    "                        except urllib.error.URLError as e:\n",
    "                            if attempts < 3:\n",
    "                                attempts += 1\n",
    "                                sleep(61)\n",
    "                                continue\n",
    "                            else:\n",
    "                                raise e\n",
    "\n",
    "                        #fetch the name\n",
    "                        name = str(payload['metadata']['name']['name'])\n",
    "\n",
    "                        #add the name to a dictionary and list\n",
    "                        tempdict[name] = domain[0]  \n",
    "                        templist.append([domain[0], name])\n",
    "                        \n",
    "                        #add the name to domaincode_name_dict to speed up the proces\n",
    "                        domaincode_name_dict[domain[1]] = name\n",
    "\n",
    "                        if next:\n",
    "                            sleep(1)\n",
    "                    sleep(1)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    tempdict[domain[1]] = domain[0]\n",
    "                    templist.append(domain)\n",
    "        \n",
    "        #create a dictionary that has the identifiers as keys and a list of lists containing the database and domain names for every found domain as values\n",
    "        domainnamedictionary[key] = tempdict\n",
    "        \n",
    "        #create a dictionary that has the identifiers as keys and a dictionary containing the names and the databases it was fetched from, as values\n",
    "        domainnamelist[key] = templist\n",
    "        \n",
    "    print(domainnamelist)\n",
    "    print(domainnamedictionary)\n",
    "    \n",
    "    return domainnamelist, domainnamedictionary\n",
    "\n",
    "#find the domain names for every protein\n",
    "print('Fetching the corresponding names for all domains.')\n",
    "finddomainnames = get_domain_name(finddomains[2])\n",
    "\n",
    "#save this output in case the script gets interrupted later\n",
    "with open(os.path.join(jobname, f\"domainnamelist{family}_{taxsubset}.fasta\"), 'w') as file:\n",
    "    file.write(str(finddomainnames[0]))\n",
    "    \n",
    "with open(os.path.join(jobname, f\"domainnamedictionary{family}_{taxsubset}.fasta\"), 'w') as file:\n",
    "    file.write(str(finddomainnames[1]))  \n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56607bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_domains_dict(positiondict, domaindict):\n",
    "    \"\"\"Creates a dictionary that contains for every entry, for every found domainpositions, the corresponding domainname\"\"\"\n",
    "    \n",
    "    alldomainsdict = {}\n",
    "    for key, value in positiondict.items():\n",
    "        tempdict = {}\n",
    "        for positions in value:\n",
    "            placeinlist = value.index(positions)\n",
    "            domainsinseq = domaindict[key]\n",
    "            domainlist = domainsinseq[placeinlist]\n",
    "            tempdict[str(positions)] = domainlist[1] \n",
    "        alldomainsdict[key] = tempdict\n",
    "                   \n",
    "    print(alldomainsdict)\n",
    "    \n",
    "    return alldomainsdict\n",
    "\n",
    "all_domains_in_seq = position_domains_dict(finddomains[1], finddomainnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec6b9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def overlap(a, b):\n",
    "    \"\"\"This function has two domains as an input. They are sorted by their position in the protein sequence.\n",
    "    If they have overlapping positions, the overlap percentage is calculated. \n",
    "    If it is above a certain cut-off, the two domains are considered to be the same\"\"\"\n",
    "    \n",
    "    #sort again\n",
    "    sortinglist = [a, b]\n",
    "    sortinglist.sort()\n",
    "    a = sortinglist[0]\n",
    "    b = sortinglist[1]\n",
    "    \n",
    "    x1, y1 = a[0], a[1]\n",
    "    length1 = y1 - x1\n",
    "    \n",
    "    x2, y2 = b[0], b[1]\n",
    "    length2 = y2 - x2\n",
    "    \n",
    "    if y1 in range(x2, y2+1):\n",
    "        #in the uncommon scenario that a 'domain' only consists of 1 AA\n",
    "        if x1 == y1 or x2 == y2:\n",
    "            identity = 1\n",
    "        else:\n",
    "            over = y1 - x2\n",
    "            identity = min(over/length1,over/length2)\n",
    "        \n",
    "    elif x2 in range (x1, y1+1) and y2 in range(x1, y1+1):\n",
    "        #this means that the second domain falls in the range of the first domain\n",
    "        #we'll consider these domains to be the same\n",
    "        identity = 100\n",
    "    \n",
    "    else:\n",
    "        identity = 0\n",
    "    \n",
    "    #if the identity between the domains is higher than this cut-off we assume they are the same domain\n",
    "    #there is no fixed value for this cut-off. The user could play around with this value and make it more/less strict\n",
    "    return(identity > 0.35)\n",
    "        \n",
    "def extract_overlap(domainlist):\n",
    "    \"\"\"This function gets a list of all the domains and their positions found in a protein sequence as an input.\n",
    "    It uses the overlap function to iterate over this list and it groups all domains we consider to be the same in a list.\n",
    "    The output is a list of lists with every sublist representing an unique domain\"\"\"\n",
    "    \n",
    "    #create a list containing the final domain composition\n",
    "    outputlist = []\n",
    "    \n",
    "    #create a temporary list that is used while iterating over all the domains, using the overlap function\n",
    "    templist = []\n",
    "    \n",
    "    #sort the inputlist according to their position in the protein sequence. N-terminus -> C-terminus\n",
    "    domainlist = sorted(domainlist, key=lambda x: (x[0], x[1]))\n",
    "    \n",
    "    i = 0\n",
    "    while i in range(len(domainlist)):\n",
    "        \n",
    "        if i == len(domainlist) - 1:\n",
    "            if len(templist) != 0:\n",
    "                #remove doubles from templist\n",
    "                templist.sort()\n",
    "                templist = list(templist for templist,_ in itertools.groupby(templist))\n",
    "                outputlist.append(templist)\n",
    "            else:\n",
    "                outputlist.append(domainlist[i])\n",
    "            break\n",
    "            \n",
    "        #if the domains are found to be the same (overlap > cut-off)\n",
    "        elif overlap(domainlist[i], domainlist[i+1]) == True:\n",
    "            templist += domainlist[i], domainlist[i+1]\n",
    "            i += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if len(templist) != 0:\n",
    "                #remove doubles from templist\n",
    "                templist.sort()\n",
    "                templist = list(templist for templist,_ in itertools.groupby(templist))\n",
    "                outputlist.append(templist)\n",
    "                templist = [] \n",
    "                \n",
    "            else:\n",
    "                outputlist.append(domainlist[i])\n",
    "            i += 1\n",
    "            \n",
    "    return outputlist\n",
    "\n",
    "def clean_domains(finddomains, alldomainsdict, databaseoutput):\n",
    "    \"\"\"This function uses the extract_overlap output as input, which is for every protein a list of sublists.\n",
    "    Every sublist represents a protein domain at a unique position in the sequence. The clean_domains function decides \n",
    "    which of the cordinates in the sublist (and corresponding domain name) is used for the final representation of the \n",
    "    proteins' domain architecture. The selection is based on database-preference.\"\"\"\n",
    "\n",
    "    \n",
    "    databasepreference = ['SMART', 'PFAM', 'PROFILE', 'GENE3D', 'SUPERFAMILY' ]\n",
    "    finaldomainoutput = {}\n",
    "    \n",
    "    #following comments show a practical example of this function\n",
    "    #finddomains[1]\n",
    "    #{'AFY45032.1': [[369, 787], [41, 372], [421, 796], [31, 801], [1, 802]], ...\n",
    "    \n",
    "    for key, val in finddomains[1].items():\n",
    "\n",
    "        #val\n",
    "        #[[369, 787], [41, 372], [421, 796], [31, 801], [1, 802]]\n",
    "        \n",
    "        numberofdomains = len(val)\n",
    "        temporarylength = 0\n",
    "       \n",
    "        #do the extract_overlap iteration several times, to otain one clean result without severely overlapping domains\n",
    "        #while length of the previous input != len(val) \n",
    "        while temporarylength != len(val):\n",
    "            \n",
    "            #length of the previous input\n",
    "            temporarylength = len(val)\n",
    "            \n",
    "            extract_overlap_dict = {}\n",
    "            \n",
    "            #use the overlap and extract_overlap functions to create a dictionary with the identifiers as keys \n",
    "            #and the unique positions containing a domains as values\n",
    "            domaincomposition = extract_overlap(val)\n",
    "            extract_overlap_dict[key] = domaincomposition\n",
    "            #{'AFY45032.1': [[[1, 802], [31, 801], [41, 372]], [[369, 787], [421, 796]]]}\n",
    "\n",
    "            for key, value in extract_overlap_dict.items():\n",
    "                #key\n",
    "                #'AFY45032.1'\n",
    "                #value\n",
    "                #[[[1, 802], [31, 801], [41, 372]], [[369, 787], [421, 796]]]\n",
    "                \n",
    "                numberofdomains = len(value)\n",
    "\n",
    "                temporaryoutput = []\n",
    "                cleanedpositiondict = {}\n",
    "                cleanedpositionlist = []\n",
    "\n",
    "                for domain in value:\n",
    "                    #domain\n",
    "                    #[[1, 802], [31, 801], [41, 372]] and [[369, 787], [421, 796]]\n",
    "                    \n",
    "                    #there can only be one position pair for this domain\n",
    "                    if all(isinstance(x, int) for x in domain):\n",
    "                        dname = alldomainsdict[key][str(domain)]\n",
    "                        db = databaseoutput[key][dname]\n",
    "\n",
    "                        #we'll not include signal peptides\n",
    "                        #also for some domains we could not retrieve a name. These domains are called 'None' and will not be included\n",
    "                        if dname != 'None' and db != 'PHOBIUS' and db != 'FUNFAM' and db.upper().find('SIGNAL') == -1: #and db != 'COILS'\n",
    "                            temporaryoutput.append([domain, dname])\n",
    "                            cleanedpositionlist.append(domain)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        #this means that we have multiple positions describing the same domain\n",
    "                        tempdict = {}\n",
    "                        tempposition = {}\n",
    "                        for simdomain in domain:\n",
    "                            \n",
    "                            #simdomain\n",
    "                            #[1, 802]\n",
    "                            dname = alldomainsdict[key][str(simdomain)]\n",
    "                            db = databaseoutput[key][dname]\n",
    "\n",
    "                            #we'll not include signal peptides\n",
    "                            if dname != 'None' and db != 'PHOBIUS' and db.upper().find('SIGNAL') == -1 and dname.upper().find('SIGNAL') == -1: #db != 'COILS'\n",
    "                                tempdict[dname] = db\n",
    "                                tempposition[dname] = simdomain\n",
    "                        \n",
    "                        #if the tempdict is not empty\n",
    "                        if bool(tempdict) == True:\n",
    "                            \n",
    "                            #iterate over the dictionary\n",
    "                            count = 1\n",
    "                            stop = 0\n",
    "\n",
    "                            for k, v in tempdict.items():\n",
    "                                while stop == 0:\n",
    "\n",
    "                                    if count == len(tempdict):\n",
    "                                        #when we reached the length of the tempdict, stop the iteration. This will be the final output\n",
    "                                        #dom = the domain name\n",
    "                                        dom = list(tempposition.keys())[0]\n",
    "                                        #create a list of lists containing the position and name of the domain\n",
    "                                        temporaryoutput.append([tempposition[dom], dom])\n",
    "                                        cleanedpositionlist.append(tempposition[dom])\n",
    "                                        stop = 1\n",
    "\n",
    "                                    else:\n",
    "                                        for i in range(len(databasepreference)): \n",
    "                                            #clean the output based on the database preference. Order matters\n",
    "\n",
    "                                            if v == databasepreference[i]:\n",
    "                                                temporaryoutput.append([tempposition[k], k])\n",
    "                                                cleanedpositionlist.append(tempposition[k])\n",
    "                                                stop = 1\n",
    "\n",
    "                                            else:\n",
    "                                                i+= 1\n",
    "\n",
    "                                        count += 1\n",
    "\n",
    "\n",
    "                finaldomainoutput[key] = temporaryoutput \n",
    "                #{'AFY45032.1': [[[1, 802], 'Beta-glucosidase, GBA2 type'], [[369, 787], 'Six-hairpin glycosidases']],...\n",
    "            \n",
    "            #update the val so the iteration can go on    \n",
    "            val = cleanedpositionlist\n",
    "                \n",
    "        finaldomainoutput[key] = temporaryoutput\n",
    "        \n",
    "    print(finaldomainoutput) \n",
    "    print('Finished!')\n",
    "    return finaldomainoutput\n",
    "\n",
    "finaldomainarchitecture = clean_domains(finddomains, all_domains_in_seq, finddomainnames[1] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819fdb4",
   "metadata": {},
   "source": [
    "# Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this step we'll curate the names of the domains\n",
    "#The user will be able to input a domain name which groups all synonymous names describing the same domain\n",
    "\n",
    "def create_domain_list(finaldomainarchitecture):\n",
    "    \"\"\"This function creates a list and dictionary, containing the several domain names that have been found\"\"\"\n",
    "    \n",
    "    #first create a list containing unique protein domain names\n",
    "    domain_list = []\n",
    "    for value in finaldomainarchitecture.values():\n",
    "        for domains in value:\n",
    "            domainname = domains[1]\n",
    "            if domainname not in domain_list:\n",
    "                domain_list.append(domainname)\n",
    "    \n",
    "    #then use the list to create a dictionary. WIth the keys being the position of every domain in the domain_list\n",
    "    found_domains_dict = {}\n",
    "    for i in range(len(domain_list)):\n",
    "        found_domains_dict[i] = domain_list[i]\n",
    "        \n",
    "    return domain_list, found_domains_dict\n",
    "\n",
    "def print_dictionary(dictionary):\n",
    "    \"\"\"This function prints out the keys and values of a dictionary\"\"\"\n",
    "                                   \n",
    "    print('\\nDomains still to curate: \\n')\n",
    "    for key, value in dictionary.items():\n",
    "        print(f'{key}: {value}')\n",
    "                                   \n",
    "    return\n",
    "\n",
    "def data_curation(finaldomainarchitecture):\n",
    "    \"\"\"This function allowd the user to curate the data\"\"\"\n",
    "    \n",
    "    unique_domains = create_domain_list(finaldomainarchitecture)\n",
    "    \n",
    "    #create a list containing unique domain names\n",
    "    domain_list = unique_domains[0]\n",
    "    #create a dictionary with the positions of the domain names in the list as keys and the names as values\n",
    "    found_domains_dict = unique_domains[1]\n",
    "    \n",
    "    print(f\"\"\"\\nLets curate the domain names!\n",
    "This step can be useful because the domains are retrieved from several databases which often have different names for the same domain. \n",
    "In the first input option, enter the domain name you want to use for the remainder of this workflow. \n",
    "Then, enter the number of the domain(s)(0 - {len(domain_list)}) you want to include in this umbrella domain name, separated by a ','. For example: \\n\n",
    "Domain name: Catalytic domain\n",
    "Includes: 0,2,5 \\n\n",
    "When finished, write 'stop'. The proces will be continued with the curated domain names.\n",
    "\"\"\")\n",
    "    \n",
    "    #store the curated domain names\n",
    "    curated_dictionary = {}\n",
    "    \n",
    "    #go over all domain names\n",
    "    for i in range(len(found_domains_dict)):\n",
    "        \n",
    "        #print which domains we still need to curate\n",
    "        print_dictionary(found_domains_dict)\n",
    "        \n",
    "        #the user can input the name for the domain \n",
    "        domainname = input('\\nDomain name: ')\n",
    "        \n",
    "        #if the user is done, all names that haven't been curated will be included in the curated_dicionary as is\n",
    "        if domainname.upper() == 'STOP':\n",
    "            for domain in found_domains_dict.values():\n",
    "                curated_dictionary[domain] = [domain]\n",
    "            break\n",
    "            \n",
    "        #the user can input which domains are synonymous \n",
    "        synonymousnames = input('Includes: ')\n",
    "        synonymousnames = synonymousnames.strip().split(',')\n",
    "        \n",
    "        #create a list containing all the synonymous domain names\n",
    "        dictvalue = []\n",
    "        for number in synonymousnames:\n",
    "            #append tthe names to the name list\n",
    "            dictvalue.append(domain_list[int(number)])\n",
    "            \n",
    "            #remove this domain name from the input dictionary\n",
    "            found_domains_dict.pop(int(number))\n",
    "        \n",
    "        #add the inputed domain name as key and synonymous name(s) as value to the output dictionary\n",
    "        curated_dictionary[domainname] = dictvalue\n",
    "    \n",
    "    return curated_dictionary\n",
    "                                   \n",
    "curated_domains = data_curation(finaldomainarchitecture)\n",
    "print(f'\\nCurated domain dictionary: {curated_domains}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d9718",
   "metadata": {},
   "source": [
    "# Write results to SQL db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090779ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sdRDM package\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/JR-1991/software-driven-rdm.git\n",
    "    \n",
    "import sdRDM\n",
    "from sdRDM import DataModel\n",
    "from sdRDM.database import build_sql_database\n",
    "from Bio import SeqIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6f49b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyeed = DataModel.from_git(\"https://github.com/AlexWindels/sdrdm-template/\")\n",
    "#visualize for each separate table\n",
    "#pyeed.ProteinSequences.visualize_tree()\n",
    "#pyeed.DomainAssemblies.visualize_tree()\n",
    "#pyeed.DomainCuration.visualize_tree()\n",
    "\n",
    "fastafile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_inclchar.fasta\")\n",
    "database = os.path.join(jobname, f\"{family}_{taxsubset}_db.db\")\n",
    "\n",
    "def parse_prot_seq(fastafile):\n",
    "    \"\"\"Parses through the fastafile containing the protein sequences\"\"\"\n",
    "    \n",
    "    with open(fastafile, 'r') as f:\n",
    "        sequences = SeqIO.parse(f, \"fasta\")\n",
    "        modified_sequences = []\n",
    "        \n",
    "        for seq_record in sequences:\n",
    "            modified_sequences.append(seq_record)\n",
    "    \n",
    "    return modified_sequences\n",
    "\n",
    "def build_db(chardict, proteinfile, domaindictionary, databasedictionary, curated_domains, database):\n",
    "\n",
    "    proteinseq = parse_prot_seq(proteinfile)\n",
    "    build_sql_database(pyeed.ProteinSequences, loc= database)\n",
    "    build_sql_database(pyeed.DomainAssemblies, loc= database)\n",
    "    build_sql_database(pyeed.DomainCuration, loc= database)\n",
    "    taxdictionary = {'B': 'Bacteria', 'A': 'Archaea', 'E': 'Eukaryota', 'V': 'Viruses', 'U': 'Unclassified'}\n",
    "    domainarchitectures = {}\n",
    "    \n",
    "    for seq_record in proteinseq:\n",
    "        \n",
    "        seq_id = seq_record.id.split('_')[0]\n",
    "        seq_org = ' '.join(seq_record.id.split('_')[1:-1])\n",
    "        seq_seq = str(seq_record.seq)\n",
    "        seq_tax = taxdictionary[seq_record.id[-1]]\n",
    "        seq_char = ''\n",
    "        seq_dom = ''\n",
    "        seq_loc = ''\n",
    "        dom_db = ''\n",
    "        \n",
    "        if seq_id in chardict:\n",
    "            seq_char = 'C'\n",
    "        \n",
    "        if seq_id in domaindictionary and seq_id in databasedictionary:\n",
    "\n",
    "            for domain in domaindictionary[seq_id]:\n",
    "                \n",
    "                domainname = domain[1]\n",
    "                \n",
    "                #retrieve the curated name\n",
    "                for umbrellaname, includednames in curated_domains.items():\n",
    "                    #go over all the names that were included in a list to the corresponding umbrella domain name (key)\n",
    "                    for name in includednames:\n",
    "                        if name == domainname:\n",
    "                            domainname = umbrellaname\n",
    "                        \n",
    "                            seq_dom += domainname + '--'\n",
    "                            \n",
    "                seq_loc += str(domain[0]) + '--'\n",
    "                \n",
    "                dom_db += databasedictionary[seq_id][domain[1]] + '--'\n",
    "                    \n",
    "        seq_dom = seq_dom.rstrip('--')\n",
    "        seq_loc = seq_loc.rstrip('--')\n",
    "        dom_db = dom_db.rstrip('--')\n",
    "        \n",
    "        if seq_dom not in domainarchitectures:\n",
    "            domainarchitectures[seq_dom] = [seq_id]\n",
    "        else:\n",
    "            domainarchitectures[seq_dom].append(seq_id)\n",
    "     \n",
    "        #build the ProteinSequence table\n",
    "        dataset = pyeed.ProteinSequences(\n",
    "        protein_sequence_id = seq_id,\n",
    "        taxonomy = seq_tax,\n",
    "        characterized = seq_char,\n",
    "        organism_name = seq_org,\n",
    "        amino_acid_sequence = seq_seq,\n",
    "        domain_architecture = seq_dom,\n",
    "        domain_locations = seq_loc,\n",
    "        domain_databases = dom_db)\n",
    "        \n",
    "        dataset.to_sql(loc= database)\n",
    "    \n",
    "    #build the ProteinDomains database\n",
    "    for key, value in domainarchitectures.items():\n",
    "        \n",
    "        domainset = pyeed.DomainAssemblies(\n",
    "            protein_domain = key,\n",
    "            protein_sequence_id = ', '.join(value))\n",
    "        \n",
    "        domainset.to_sql(loc= database)\n",
    "    \n",
    "    #build the DomainCuration database\n",
    "    for key, value in curated_domains.items():\n",
    "        curationset = pyeed.DomainCuration(\n",
    "        domain_name = key,\n",
    "        synonymous_domain_name = ', '.join(value))\n",
    "        \n",
    "        curationset.to_sql(loc= database)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "buildb = build_db(blastchar[0],fastafile, finaldomainarchitecture, finddomainnames[1], curated_domains, database)\n",
    "print('Building the database finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c242ff",
   "metadata": {},
   "source": [
    "# Exclude sequences without domain result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f041eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only perform MSA and PTI for sequences which contain at least 1 domain\n",
    "\n",
    "inputfile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_inclchar.fasta\")\n",
    "charseq = os.path.join(jobname, f\"Characterized_{family}_{taxsubset}_FASTA_sequences.fasta\")\n",
    "outputfile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_inclchar_selected.fasta\")\n",
    "\n",
    "def select_sequences(inputfile, characterizedseq, domaindictionary, execute = True):\n",
    "    \"\"\"This function creates an outputfile containing sequences that contain at least one domain.\"\"\"\n",
    "   \n",
    "    no_domains = []\n",
    "    output = ''\n",
    "    for key,value in domaindictionary.items():\n",
    "        if value == []:\n",
    "            no_domains.append(key)\n",
    "    \n",
    "    for seq_record in SeqIO.parse(inputfile, 'fasta'):\n",
    "        identifier = seq_record.id[:seq_record.id.find('.')+2]\n",
    "        if identifier not in no_domains:\n",
    "            output += '>' + seq_record.id + '\\n'+ seq_record.seq + '\\n'\n",
    "            \n",
    "    for seq_record in SeqIO.parse(characterizedseq, 'fasta'):\n",
    "        identifier = seq_record.id[:seq_record.id.find('.')+2]\n",
    "        if identifier in no_domains:  \n",
    "            print(f'Warning: Following characterized protein will be excluded from subsequent analysis {identifier}.')\n",
    "   \n",
    "    return output, no_domains\n",
    "\n",
    "domaincontainingseq = select_sequences(inputfile, charseq, finaldomainarchitecture, execute)\n",
    "with open(outputfile, 'w') as file:\n",
    "    file.write(str(domaincontainingseq[0]))\n",
    "\n",
    "print(f'{sizefastafile(outputfile)} sequences will be used for MSA and PTI.')\n",
    "print(f'For following proteins no domains were found: {domaincontainingseq[1]}. These will be excluded from the subsequent analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa04b37",
   "metadata": {},
   "source": [
    "# MSA: MAFFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "import os\n",
    "\n",
    "from subprocess import Popen, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAFFT\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def MAFFT(execute = True):\n",
    "    \"\"\"When execute is True, this function runs the Clustal Omega executable to perform a MSA\"\"\"\n",
    "    if execute: \n",
    "        \n",
    "        #path to clustalo_exe\n",
    "        mafft = os.getcwd() + '/mafft'\n",
    "        \n",
    "        #name of the input file\n",
    "        in_file = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_inclchar_selected.fasta\")\n",
    "\n",
    "        #name you want to give to the outputfile\n",
    "        out_file = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_aligned.fasta\")\n",
    "        \n",
    "        #perform the multiple sequence alignment using MAFFT\n",
    "        mafft_cmd = [\"mafft\", \"--auto\", in_file]\n",
    "        with open(out_file, \"w\") as outfile:\n",
    "            p = subprocess.Popen(mafft_cmd, stdout=outfile, stderr=subprocess.PIPE)\n",
    "            _, error_output = p.communicate()\n",
    "        if p.returncode != 0:\n",
    "            raise RuntimeError(\"An error occurred while running MAFFT:\\n{}\".format(error_output.decode()))\n",
    "    \n",
    "    with open(out_file, 'r') as r:\n",
    "        print(r.read())\n",
    "        \n",
    "    print('Finished!')\n",
    "    return\n",
    "\n",
    "alignment = MAFFT(execute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af648417",
   "metadata": {},
   "source": [
    "# Phylogenetic tree: FastTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Phylo\n",
    "from Bio.Phylo.PhyloXML import Phylogeny\n",
    "from io import StringIO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def FastTree(execute = True):\n",
    "    \"\"\"When execute is True, this function runs the FastTree executable to perform PTI\"\"\"\n",
    "    \n",
    "    if execute:\n",
    "        \n",
    "        #path to FastTree_exe\n",
    "        FastTree = os.getcwd() + '/FastTree'\n",
    "        \n",
    "        #name of the inputfile\n",
    "        inputfile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_aligned.fasta\")\n",
    "        \n",
    "        #name of the outputfile\n",
    "        outputfile = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_phyltree.nwk\")\n",
    "        \n",
    "        #build a phylogenetic tree using FastTree\n",
    "        fasttree_process = Popen([FastTree, \"-quiet\", \"-out\", outputfile, inputfile], stdout=PIPE, stderr=PIPE)\n",
    "        stdout, stderr = fasttree_process.communicate()\n",
    "        \n",
    "        print(f\"{outputfile} finished!\")\n",
    "    \n",
    "    return \n",
    "\n",
    "phylogenetictree = FastTree(execute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac0317",
   "metadata": {},
   "source": [
    "# Domain visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This can only be executed when a phylogenetic tree is constructed\n",
    "import ete3\n",
    "from ete3 import Tree, SeqMotifFace, TreeStyle, add_face_to_node, TextFace\n",
    "\n",
    "phyltree = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_phyltree.nwk\")\n",
    "proteinseq = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_inclchar_selected.fasta\")\n",
    "\n",
    "def parse_prot_seq(fastafile):\n",
    "    \"\"\"Parses through the fastafile containing the protein sequences\"\"\"\n",
    "    \n",
    "    with open(fastafile, 'r') as f:\n",
    "        sequences = SeqIO.parse(f, \"fasta\")\n",
    "        modified_sequences = []\n",
    "        \n",
    "        for seq_record in sequences:\n",
    "            modified_sequences.append(seq_record)\n",
    "    \n",
    "    return modified_sequences\n",
    "    \n",
    "def get_motifs(protein, curated_domains, shapedict, colordict):\n",
    "    \"\"\"This function gives a shape and color to each domain found in the protein sequences. It creates a protein motif\n",
    "    that annotates the phylogenetic tree\"\"\"\n",
    "    numberofshapes = len(shapedict)\n",
    "    numberofcolors = len(colordict)\n",
    "    \n",
    "    shapes = ['[]', 'o', '()', '^', '<>', 'v']\n",
    "    colors =  ['red', 'green', 'blue', 'yellow', 'purple', 'orange', 'pink', 'cyan', 'magenta', 'gray', 'black']\n",
    "    \n",
    "    proteinmotif = []\n",
    "    \n",
    "    for key, value in protein.items():\n",
    "        for domains in value:\n",
    "            domainname = domains[1]\n",
    "            \n",
    "            #retrieve the curated name\n",
    "            for umbrellaname, includednames in curated_domains.items():\n",
    "                \n",
    "                #go over all the names that were included in a list to the corresponding umbrella domain name (key)\n",
    "                for name in includednames:\n",
    "                    if name == domainname:\n",
    "                        domainname = umbrellaname\n",
    "                        \n",
    "            domainstart = domains[0][0]\n",
    "            domainend = domains[0][1]\n",
    "            \n",
    "            if domainname not in shapedict:\n",
    "                shapedict[domainname] = shapes[numberofshapes%len(shapes)]\n",
    "                \n",
    "            if domainname not in colordict:\n",
    "                colordict[domainname] = colors[numberofcolors%len(colors)]\n",
    "            \n",
    "            # [seq.start, seq.end, shape, width, height, fgcolor, bgcolor]\n",
    "            domainmotif = [\n",
    "                domainstart, domainend, shapedict[domainname], None, 15, colordict[domainname], colordict[domainname],f'arial|20|white|{domainname}']\n",
    "           \n",
    "            proteinmotif.append(domainmotif)\n",
    "\n",
    "\n",
    "    return proteinmotif\n",
    "\n",
    "def domain_vis(tree, finaldomainoutput, curated_domains, proteinsequences, execute = True):\n",
    "    \"\"\"Function gathers all protein motifs and assembles them to annotate the phylogenetic tree\"\"\"\n",
    "    \n",
    "    if execute:\n",
    "    \n",
    "        protseq = parse_prot_seq(proteinsequences)\n",
    "        seqmotifsdict = {}\n",
    "\n",
    "        #inputtree\n",
    "        t = Tree(tree)\n",
    "\n",
    "        shapedict = {}\n",
    "        colordict = {}\n",
    "\n",
    "        for key, value in finaldomainoutput.items():\n",
    "\n",
    "            domotif = get_motifs({key:value},curated_domains, shapedict, colordict)\n",
    "\n",
    "            for seq_record in protseq:\n",
    "\n",
    "                if seq_record.id.find('_') >2: \n",
    "                    proteinidentifier = seq_record.id.split('_')[0]\n",
    "\n",
    "                else:\n",
    "                    proteinidentifier = seq_record.id.split('_')[0]+'_'+seq_record.id.split('_')[1]\n",
    "                    #print(proteinidentifier)\n",
    "\n",
    "                if proteinidentifier == key:\n",
    "\n",
    "                    seq = str(seq_record.seq)\n",
    "                    leaf = seq_record.id\n",
    "                    seqmotifsdict[leaf] = domotif\n",
    "\n",
    "                    seqFace = SeqMotifFace(seq, motifs=domotif, seq_format=\"-\")\n",
    "                    (t & leaf).add_face(seqFace, 0, \"aligned\")\n",
    "        \n",
    "        return t\n",
    "\n",
    "\n",
    "domainvisualisation_in_tree = domain_vis(phyltree, finaldomainarchitecture, curated_domains, proteinseq, execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbee687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ete3_visualisation(domainvisualisation, execute = True):\n",
    "    \"\"\"This function opens a ete3 browser tab containing the constructed phylogenetic tree,\n",
    "    annotated with the domain architecture of every protein. This function is executed automatically when \n",
    "    PTI is selected at the input options. \n",
    "    WARNING: the browser tab needs to be closed again before the workflow can be resumed.\"\"\"\n",
    "    \n",
    "    if execute:\n",
    "        \n",
    "        t = domainvisualisation_in_tree\n",
    "\n",
    "\n",
    "        ts = TreeStyle()\n",
    "        ts.show_leaf_name = True\n",
    "\n",
    "        ts.tree_width = 100\n",
    "        ts.branch_vertical_margin = 15\n",
    "\n",
    "        #circular representation \n",
    "        ts.mode = \"c\"\n",
    "        ts.arc_start = -180 # 0 degrees = 3 o'clock\n",
    "        ts.arc_span = 359\n",
    "\n",
    "        #add title\n",
    "        ts.title.add_face(TextFace(f\"{family} Phylogenetic Tree\", fsize=20), column=0)\n",
    "\n",
    "        t.show(tree_style=ts)\n",
    "        \n",
    "    return\n",
    "\n",
    "ete3_vis = ete3_visualisation(domainvisualisation_in_tree, execute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac5539",
   "metadata": {},
   "source": [
    "# Create iTOL annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary modules\n",
    "from ete3 import Tree, TreeStyle, TextFace, NodeStyle\n",
    "\n",
    "phyltree = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_phyltree.nwk\")\n",
    "proteinseq = os.path.join(jobname, f\"CAZy_{family}_{taxsubset}_{cutoff}_inclchar_selected.fasta\")\n",
    "iTOL_annotation_file = os.path.join(jobname, f\"iTOL_annotation_CAZy_{family}_{taxsubset}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0caa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prot_seq(fastafile):\n",
    "    \"\"\"Parses through the fastafile containing the protein sequences\"\"\"\n",
    "    \n",
    "    with open(fastafile, 'r') as f:\n",
    "        sequences = SeqIO.parse(f, \"fasta\")\n",
    "        modified_sequences = []\n",
    "        \n",
    "        for seq_record in sequences:\n",
    "            modified_sequences.append(seq_record)\n",
    "    \n",
    "    return modified_sequences\n",
    "    \n",
    "def get_motifs(protein, curated_domains, shapedictionary, colordictionary):\n",
    "    \"\"\"This function gives a shape and color to each domain found in the protein sequences. It creates a protein motif\n",
    "    that annotates the phylogenetic tree in iTOL\"\"\"\n",
    "    \n",
    "    shapes = ['RE', 'EL', 'HH', 'HV', 'DI', 'TR', 'TL', 'PL', 'PR', 'PU', 'PD', 'OC']\n",
    "    colors =  ['#BF0F0F', '#217D3B', '#23A5D9', '#F2C641', '#A276DB', '#D9560B', '#F294B6', '#1DF2DD', '#E80C7A', '#A6A6A6', '#000000']\n",
    "    \n",
    "    proteinmotif = ''\n",
    "    shapedict = shapedictionary\n",
    "    colordict = colordictionary\n",
    "    \n",
    "    for key, value in protein.items():\n",
    "        for domains in value:\n",
    "            domainname = domains[1]\n",
    "            \n",
    "            #retrieve the curated name\n",
    "            for umbrellaname, includednames in curated_domains.items():\n",
    "                \n",
    "                #go over all the names that were included in a list to the corresponding umbrella domain name (key)\n",
    "                for name in includednames:\n",
    "                    if name == domainname:\n",
    "                        domainname = umbrellaname\n",
    "                        \n",
    "            domainstart = domains[0][0]\n",
    "            domainend = domains[0][1]\n",
    "            \n",
    "            numberofshapes = len(shapedict)\n",
    "            numberofcolors = len(colordict)\n",
    "            \n",
    "            if domainname not in shapedict:\n",
    "                shapedict[domainname] = shapes[numberofshapes%len(shapes)]\n",
    "                \n",
    "            if domainname not in colordict:\n",
    "                colordict[domainname] = colors[numberofcolors%len(colors)]\n",
    "            \n",
    "            # shapedict[domainname]|domainstart|domainend|colordict[domainname]|domainname\n",
    "            domainmotif = f'{shapedict[domainname]}|{domainstart}|{domainend}|{colordict[domainname]}|{domainname}'\n",
    "            \n",
    "           \n",
    "            proteinmotif += '\\t' + domainmotif\n",
    "\n",
    "    return proteinmotif\n",
    "\n",
    "def domain_vis(tree, finaldomainoutput, curated_domains, proteinsequences, execute = True):\n",
    "    \"\"\"Function gathers all protein motifs and assembles them to annotate the phylogenetic tree\"\"\"\n",
    "    \n",
    "    protseq = parse_prot_seq(proteinsequences)\n",
    "    seqmotifsdict = {}\n",
    "    \n",
    "    #inputtree\n",
    "    t = Tree(tree)\n",
    "    \n",
    "    shapedictionary = {}\n",
    "    colordictionary = {}\n",
    "    iTOL_output = ''\n",
    "    iTOL_file = f\"\"\"DATASET_DOMAINS\n",
    "SEPARATOR TAB\n",
    "DATASET_LABEL\tSMART architecture export\n",
    "COLOR\t#0000ff\n",
    "BORDER_WIDTH\t1\n",
    "GRADIENT_FILL\t1\n",
    "SHOW_DOMAIN_LABELS\t1\n",
    "#Check iTOL help page for further dataset options\n",
    "LEGEND_TITLE\t {family}_{taxsubset} domains\n",
    "\"\"\"\n",
    "    \n",
    "    for key, value in finaldomainoutput.items():\n",
    "        \n",
    "        domotif = get_motifs({key:value},curated_domains, shapedictionary, colordictionary)\n",
    "        \n",
    "        for seq_record in protseq:\n",
    "            \n",
    "            if seq_record.id.find('_') >2: \n",
    "                proteinidentifier = seq_record.id.split('_')[0]\n",
    "                \n",
    "            else:\n",
    "                proteinidentifier = seq_record.id.split('_')[0]+'_'+seq_record.id.split('_')[1]\n",
    "\n",
    "            if proteinidentifier == key:\n",
    "               \n",
    "                len_seq = len(str(seq_record.seq))\n",
    "                leaf = seq_record.id\n",
    "                \n",
    "                iTOL_output += str(leaf) + '\\t' + str(len_seq) + '\\t' + domotif + '\\n'\n",
    "                \n",
    "            \n",
    "    iTOL_file += 'LEGEND_LABELS' + '\\t' + '\\t'        \n",
    "    for key, value in shapedictionary.items():\n",
    "        iTOL_file += key + '\\t'\n",
    "        \n",
    "    iTOL_file += '\\n' + 'LEGEND_SHAPES' + '\\t' + '\\t'        \n",
    "    for key, value in shapedictionary.items():\n",
    "        iTOL_file += value + '\\t'\n",
    "        \n",
    "    iTOL_file += '\\n' + 'LEGEND_COLORS' + '\\t' + '\\t'        \n",
    "    for key, value in shapedictionary.items():\n",
    "        iTOL_file += colordictionary[key] + '\\t'\n",
    "        \n",
    "    iTOL_file += '\\n' + 'DATA' + '\\n' + iTOL_output\n",
    "    \n",
    "    #print(shapedictionary)\n",
    "    #print(colordictionary)\n",
    "    print(iTOL_file)\n",
    "    return iTOL_file\n",
    "\n",
    "def characterized_labeling(phyltree, characterizedECnumbers, execute = True):\n",
    "    #inputtree\n",
    "    t = Tree(phyltree)\n",
    "    \n",
    "    with open(os.path.join(jobname, f\"Characterized_{family}_GenBankID_file.txt\"), 'r') as file:\n",
    "        \n",
    "        colors =  ['#BF0F0F', '#217D3B', '#23A5D9', '#F2C641', '#A276DB', '#D9560B', '#F294B6', '#1DF2DD', '#E80C7A', '#A6A6A6']\n",
    "        colordict = {}\n",
    "        outputfile = f\"\"\"DATASET_STYLE\n",
    "SEPARATOR SPACE\n",
    "DATASET_LABEL Characterized enzymes\n",
    "COLOR #0000ff\n",
    "BORDER_WIDTH 1\n",
    "GRADIENT_FILL 1\n",
    "SHOW_DOMAIN_LABELS 1\n",
    "#Check iTOL help page for further dataset options\n",
    "LEGEND_TITLE {family}_{taxsubset} characterized enzymes\n",
    "\"\"\"\n",
    "        tempoutput = ''\n",
    "        for ID in file:\n",
    "            ID = ID.strip()\n",
    "            if ID in characterizedECnumbers:\n",
    "                numberofcolors = len(colordict)\n",
    "                if characterizedECnumbers[ID] not in colordict:\n",
    "                    colordict[characterizedECnumbers[ID]] = colors[numberofcolors%len(colors)]\n",
    "                \n",
    "            #color the labels of characterized sequences according to their activity\n",
    "            for node in t.traverse():\n",
    "                if node.is_leaf():\n",
    "                    if ID in node.name:\n",
    "                        tempoutput += node.name + ' ' + 'label'+ ' ' + 'node' + ' '+ str(colordict[characterizedECnumbers[ID]]) + ' ' +  '1' + ' ' + 'bold' + '\\n'\n",
    "    \n",
    "    outputfile += 'LEGEND_LABELS' + ' '        \n",
    "    for key, value in colordict.items():\n",
    "        outputfile += key + ' '\n",
    "    \n",
    "    outputfile += '\\n' + 'LEGEND_SHAPES' + ' ' \n",
    "    count = 1\n",
    "    for key, value in colordict.items():\n",
    "        outputfile += str(count) + ' '\n",
    "        count += 1\n",
    "        \n",
    "    outputfile += '\\n' + 'LEGEND_COLORS' + ' '        \n",
    "    for key, value in colordict.items():\n",
    "        outputfile += colordict[key] + ' '\n",
    "    \n",
    "    outputfile += '\\n' + 'DATA' + '\\n' + tempoutput \n",
    "    \n",
    "    print(colordict)\n",
    "    print(outputfile)\n",
    "    return outputfile\n",
    "\n",
    "iTOL = domain_vis(phyltree, finaldomainarchitecture, curated_domains, proteinseq, execute)\n",
    "characterized_annotation = characterized_labeling(phyltree, characterizedECnumbers, execute)\n",
    "\n",
    "if execute: \n",
    "    \n",
    "    with open(iTOL_annotation_file, 'w') as f:\n",
    "        f.write(iTOL)\n",
    "\n",
    "    #create an annotation file so that the leaf labels of characterized enzymes are colored by activity\n",
    "    with open(os.path.join(jobname, f\"iTOL_annotation_CAZy_{family}_{taxsubset}_characterized_enzymes.txt\"),'w') as r:\n",
    "        r.write(characterized_annotation)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw-celnotatie",
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
